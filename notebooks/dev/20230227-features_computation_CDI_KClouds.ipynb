{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b43f7c-9eb9-473c-bd6a-388ac880ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4437c6-4444-4756-ab50-5855e0b21003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04431e6a-a61f-431a-839d-3d3648b408bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import atexit\n",
    "\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "import azure.storage.blob\n",
    "import rioxarray\n",
    "import rasterio\n",
    "import stackstac\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask_image.ndmorph as ndmorph\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import skimage\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler, RobustScaler\n",
    "\n",
    "import satio_pc\n",
    "from satio_pc import parallelize\n",
    "from satio_pc.clouds import scl_to_mask\n",
    "\n",
    "\n",
    "# compositing code\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse as parse_date\n",
    "\n",
    "\n",
    "def _get_date_range(start, end, freq, window):\n",
    "    \n",
    "    before = round(window / 2)\n",
    "    start, end = parse_date(start), parse_date(end)\n",
    "\n",
    "    date_range = pd.date_range(start=start + timedelta(days=before),\n",
    "                               end=end,\n",
    "                               freq=f'{freq}D')\n",
    "    return date_range\n",
    "\n",
    "\n",
    "def calculate_moving_composite(darr: xr.DataArray,\n",
    "                               freq=7,\n",
    "                               window=None,\n",
    "                               start=None,\n",
    "                               end=None,\n",
    "                               use_all_obs=False):\n",
    "    \"\"\"\n",
    "    Calculate moving median composite of an hyper cube with shape [bands, time,\n",
    "    rows, cols]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arrs : Numpy 4d array [bands, time, rows, cols]\n",
    "\n",
    "    start : str or datetime\n",
    "        start date for the new composites dates\n",
    "    end : str or datetime\n",
    "        end date for the new composites dates\n",
    "    freq : int\n",
    "        days interval for new dates array\n",
    "    window : int\n",
    "        moving window size in days on which the compositing function is applied\n",
    "    mode : string\n",
    "        compositing mode. Should be one of 'median', 'mean', 'sum',\n",
    "        'min' or 'max'\n",
    "    use_all_obs : bool\n",
    "        When compositing, the last window might be less than window/2 days\n",
    "        (or freq/2 if window is None). In this case, some observations might\n",
    "        get discarded from the compositing function, as the window length\n",
    "        would be too short. Setting this `True` will include the last\n",
    "        observations in the last available window, which will then span more\n",
    "        days than the `window` value. This would avoid discarding observations\n",
    "        which would be used to increase the SNR of the last window but losing\n",
    "        temporal resolution.\n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    Tuple of time_vector and composite 4d array\n",
    "    \"\"\"\n",
    "    window = window or freq  # if window is None, default to `freq` days\n",
    "\n",
    "    if window < freq:\n",
    "        raise ValueError('`window` value should be equal or greater than '\n",
    "                         '`freq` value.')\n",
    "\n",
    "    before, after = _get_before_after(window)\n",
    "    date_range = _get_date_range(start, end, freq, before)\n",
    "\n",
    "    comp_shape = (darr.shape[0], len(date_range),\n",
    "                  darr.shape[2], darr.shape[3])\n",
    "\n",
    "    comp = da.zeros(comp_shape,\n",
    "                    chunks=(1, 1, comp_shape[2], comp_shape[3]),\n",
    "                    dtype=darr.dtype)\n",
    "    # comp = da.zeros(comp_shape,\n",
    "    #                 dtype=darr.dtype)\n",
    "    time = darr.time.values\n",
    "    \n",
    "    start = str(time[0])[:10] if start is None else start\n",
    "    end = str(time[-1])[:10] if end is None else end\n",
    "\n",
    "    date_range = _get_date_range(start, end, freq, before)\n",
    "\n",
    "    comp_shape = (len(date_range), darr.shape[1],\n",
    "                  darr.shape[2], darr.shape[3])\n",
    "\n",
    "    comp = da.zeros(comp_shape,\n",
    "                    chunks=(1, 1, darr.shape[2], darr.shape[3]),\n",
    "                    dtype=darr.dtype)\n",
    "\n",
    "    time = darr.time.values\n",
    "    intervals_flags = _get_invervals_flags(date_range,\n",
    "                                           time,\n",
    "                                           before,\n",
    "                                           after,\n",
    "                                           use_all_obs)\n",
    "    \n",
    "    # comp = []\n",
    "    for i, d in enumerate(date_range):\n",
    "        flag = intervals_flags[i]\n",
    "        idxs = np.where(flag)[0]\n",
    "        \n",
    "        band_arrs = []\n",
    "        for band_idx in range(comp.shape[1]):\n",
    "            comp[i, band_idx, ...] = nanmedian(darr.isel(time=idxs,\n",
    "                                                         band=band_idx))\n",
    "        \n",
    "    darr_out = xr.DataArray(comp,\n",
    "                            dims=darr.dims,\n",
    "                            coords={'time': date_range.values,\n",
    "                                    'band': darr.band,\n",
    "                                    'y': darr.y,\n",
    "                                    'x': darr.x},\n",
    "                            attrs=darr.attrs)\n",
    "                                               \n",
    "    return darr_out\n",
    "\n",
    "\n",
    "def _include_last_obs(idxs):\n",
    "\n",
    "    # check that all obs are used on last interval\n",
    "    true_flags = np.where(idxs)[0]\n",
    "    if true_flags.size:\n",
    "        last_true_idx = np.where(idxs)[0][-1]\n",
    "        if last_true_idx != idxs.size - 1:\n",
    "            idxs[last_true_idx:] = True\n",
    "\n",
    "    return idxs\n",
    "\n",
    "\n",
    "def _get_invervals_flags(date_range,\n",
    "                         time_vector,\n",
    "                         before,\n",
    "                         after,\n",
    "                         use_all_obs):\n",
    "\n",
    "    intervals_flags = []\n",
    "    for i, d in enumerate(date_range):\n",
    "        idxs = interval_flag(\n",
    "            pd.to_datetime(time_vector),\n",
    "            d,\n",
    "            before=before,\n",
    "            after=after)\n",
    "\n",
    "        if (i == len(date_range) - 1) and use_all_obs:\n",
    "            idxs = _include_last_obs(idxs)\n",
    "\n",
    "        intervals_flags.append(idxs)\n",
    "\n",
    "    return intervals_flags\n",
    "\n",
    "\n",
    "def nanmedian(arr):\n",
    "    \"\"\"arr should be an xarray with dims (time, y, x)\"\"\"\n",
    "    \n",
    "    start_dtype = arr.dtype\n",
    "    if start_dtype not in (np.float32, np.float64):\n",
    "        arr = arr.astype(np.float32)\n",
    "        arr = arr.where(arr != 0, np.nan)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "        res = da.nanmedian(arr.data, axis=0)\n",
    "\n",
    "    # res comes out as float. nans will be casted to 0s when returning to int\n",
    "    return res.astype(start_dtype)\n",
    "\n",
    "\n",
    "def _get_before_after(window: int):\n",
    "    \"\"\"\n",
    "    Returns values for before and after in number of days, given a\n",
    "    window length.\n",
    "    \"\"\"\n",
    "    half, mod = window // 2, window % 2\n",
    "\n",
    "    before = after = half\n",
    "\n",
    "    if mod == 0:  # even window size\n",
    "        after = max(0, after - 1)  # after >= 0\n",
    "\n",
    "    return before, after\n",
    "\n",
    "\n",
    "def _check_window_settings(freq,\n",
    "                           before,\n",
    "                           after,\n",
    "                           mode,\n",
    "                           supported_modes=None):\n",
    "    \"\"\"\n",
    "    Perform a check on the values of before and after against freq, mode\n",
    "    and supported_modes.\n",
    "\n",
    "    If mode is sum before and after will be computed to give non overlapping\n",
    "    windows.\n",
    "\n",
    "    If one of before or after are None, the other is set to the values of the\n",
    "    valid one.\n",
    "\n",
    "    If before and after are both None,\n",
    "    \"\"\"\n",
    "    if supported_modes is None:\n",
    "        supported_modes = SUPPORTED_MODES\n",
    "\n",
    "    if mode not in supported_modes:\n",
    "        raise ValueError(('Compositing mode should be one of '\n",
    "                          f'{supported_modes}, but got: `{mode}`'))\n",
    "\n",
    "    no_ov_modes = ['sum', 'min', 'max']\n",
    "    if mode in no_ov_modes:\n",
    "        if (before is not None or after is not None):\n",
    "            logger.warning(('`before` and `after` arguments are ignored for '\n",
    "                            f'compositing mode `{no_ov_modes}`.'))\n",
    "        # For these modes window overlap is not allowed in the time subsets\n",
    "        # to avoid double counting of values\n",
    "        before = after = None\n",
    "\n",
    "    # if one of before or after is None, set it simmetrically to the valid one\n",
    "    before = before or after\n",
    "    after = after or before\n",
    "\n",
    "    if (before is None and after is None):\n",
    "        before, after = _get_default_before_after(freq)\n",
    "\n",
    "    return before, after\n",
    "\n",
    "\n",
    "def _get_default_before_after(freq):\n",
    "    \"\"\"\n",
    "    Based on the freq, return before and after for non overlapping windows\n",
    "    \"\"\"\n",
    "    if freq == 1:\n",
    "        before = 0\n",
    "        after = 0\n",
    "    elif freq % 2 == 0:\n",
    "        before = freq / 2 - 1\n",
    "        after = freq / 2\n",
    "    else:\n",
    "        before = int(np.floor(freq / 2))\n",
    "        after = int(np.floor(freq / 2))\n",
    "\n",
    "    return before, after\n",
    "\n",
    "\n",
    "def interval_flag(time_vector,\n",
    "                  date: datetime,\n",
    "                  before: int,\n",
    "                  after: int):\n",
    "    \"\"\"\n",
    "    Returns a boolean array True where the dates in time_vector fall in an\n",
    "    interval between data - before and date + after\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time_vector : datetime/np.datetime64 array\n",
    "        time_vector of the source.\n",
    "    date : datetime/np.datetime64\n",
    "        target date from which we want to get the neighboring dates from\n",
    "        time_vector\n",
    "    \"\"\"\n",
    "    midnight = datetime(date.year, date.month, date.day)\n",
    "    return ((time_vector >= midnight + timedelta(days=-before))\n",
    "            & (time_vector < midnight + timedelta(days=after + 1)))\n",
    "\n",
    "\n",
    "def mask_clouds(darr, scl_mask):\n",
    "    \"\"\"darr has dims (time, band, y, x),\n",
    "    mask has dims (time, y, x)\"\"\"\n",
    "    mask = da.broadcast_to(scl_mask.data, darr.shape)\n",
    "    darr_masked = da.where(~mask, 0, darr)\n",
    "    return darr_masked\n",
    "\n",
    "\n",
    "def force_unique_time(darr):\n",
    "    \"\"\"Add microseconds to time vars which repeats in order to make the\n",
    "    time index of the DataArray unique, as sometimes observations from the same\n",
    "    day can be split in multiple obs\"\"\"\n",
    "    unique_ts, counts_ts = np.unique(darr.time, return_counts=True)\n",
    "    double_ts = unique_ts[np.where(counts_ts > 1)]\n",
    "\n",
    "    new_time = []\n",
    "    c = 0\n",
    "    for i in range(darr.time.size):\n",
    "        v = darr.time[i].values\n",
    "        if v in double_ts:\n",
    "            v = v + c\n",
    "            c += 1\n",
    "        new_time.append(v)\n",
    "    new_time = np.array(new_time)\n",
    "    darr['time'] = new_time\n",
    "    return darr\n",
    "\n",
    "\n",
    "def harmonize(data):\n",
    "    \"\"\"\n",
    "    Harmonize new Sentinel-2 data to the old baseline.\n",
    "    \n",
    "    https://planetarycomputer.microsoft.com/dataset/sentinel-2-l2a#Baseline-Change\n",
    "    https://github.com/microsoft/PlanetaryComputer/issues/134\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: xarray.DataArray\n",
    "        A DataArray with four dimensions: time, band, y, x\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    harmonized: xarray.DataArray\n",
    "        A DataArray with all values harmonized to the old\n",
    "        processing baseline.\n",
    "    \"\"\"\n",
    "    baseline = data.coords['s2:processing_baseline'].astype(float)\n",
    "    baseline_flag = baseline < 4\n",
    "    \n",
    "    if all(baseline_flag):\n",
    "        return data\n",
    "    \n",
    "    offset = 1000\n",
    "    bands = [\"B01\", \"B02\", \"B03\", \"B04\",\n",
    "             \"B05\", \"B06\", \"B07\", \"B08\",\n",
    "             \"B8A\", \"B09\", \"B10\", \"B11\", \"B12\"]\n",
    "\n",
    "    old = data.isel(time=baseline_flag)\n",
    "    to_process = list(set(bands) & set(data.band.data.tolist()))\n",
    "    \n",
    "    new = data.sel(time=~baseline_flag).drop_sel(band=to_process)\n",
    "\n",
    "    new_harmonized = data.sel(time=~baseline_flag, band=to_process).copy()\n",
    "    \n",
    "    new_harmonized = new_harmonized.clip(offset)\n",
    "    new_harmonized -= offset\n",
    "\n",
    "    new = xr.concat([new, new_harmonized], \"band\").sel(band=data.band.data.tolist())\n",
    "    return xr.concat([old, new], dim=\"time\")\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "def _rescale_ts(ts,\n",
    "                scale=2,\n",
    "                order=1,\n",
    "                preserve_range=True,\n",
    "                nodata_val=0,\n",
    "                sigma=0.5):\n",
    "    \n",
    "    if order > 1:\n",
    "        raise ValueError('Skimage giving issues with nans and cubic interp')\n",
    "    \n",
    "    ts_dtype = ts.dtype\n",
    "    \n",
    "    if order > 0:    \n",
    "        new_dtype = np.float32\n",
    "        ts = ts.astype(new_dtype)\n",
    "        ts[ts == nodata_val] = np.nan\n",
    "    else:\n",
    "        new_dtype = ts_dtype\n",
    "\n",
    "    shape = ts.shape\n",
    "    new_shape = shape[0], shape[1], int(shape[2] * scale), int(shape[3] * scale)\n",
    "    new = np.empty(new_shape, dtype=new_dtype)\n",
    "    \n",
    "    anti_aliasing = None\n",
    "    anti_aliasing_sigma = None\n",
    "    \n",
    "    if scale < 1:\n",
    "        anti_aliasing = True,\n",
    "        anti_aliasing_sigma = sigma\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for t in range(shape[0]):\n",
    "            new[t, :, :, :] = skimage.transform.rescale(ts[t, ...],\n",
    "                                                        scale=scale,\n",
    "                                                        order=order,\n",
    "                                                        preserve_range=preserve_range,\n",
    "                                                        channel_axis=0,\n",
    "                                                        anti_aliasing=anti_aliasing,\n",
    "                                                        anti_aliasing_sigma=anti_aliasing_sigma)\n",
    "\n",
    "    new[da.isnan(new)] = 0\n",
    "    new = new.astype(ts_dtype)\n",
    "\n",
    "    return new\n",
    "\n",
    "\n",
    "def rescale_ts(ds20,\n",
    "               scale=2,\n",
    "               order=1,\n",
    "               preserve_range=True,\n",
    "               nodata_val=0,\n",
    "               sigma=0.5):\n",
    "    \n",
    "    chunks = list(ds20.chunks)\n",
    "    \n",
    "    for i in -1, -2:\n",
    "        chunks[i] = tuple(map(lambda x: x * scale, chunks[i]))\n",
    "    \n",
    "    darr_scaled = da.map_blocks(\n",
    "        _rescale_ts,\n",
    "        ds20.data,\n",
    "        dtype=ds20.dtype,\n",
    "        chunks=chunks,\n",
    "        scale=scale,\n",
    "        order=order,\n",
    "        preserve_range=preserve_range,\n",
    "        nodata_val=nodata_val,\n",
    "        sigma=sigma)\n",
    "\n",
    "    xmin, ymin, xmax, ymax = ds20.satio.bounds\n",
    "    new_res = ds20.attrs.get('resolution',\n",
    "                             ds20.x[1] - ds20.x[0]) / scale\n",
    "    new_res_half = new_res / 2\n",
    "    \n",
    "    new_x = np.linspace(xmin + new_res_half,\n",
    "                        xmax - new_res_half,\n",
    "                        int(ds20.shape[-2] * scale))\n",
    "    \n",
    "    new_y = np.linspace(ymax - new_res_half,\n",
    "                        ymin + new_res_half,\n",
    "                        int(ds20.shape[-1] * scale))\n",
    "\n",
    "    ds20u = xr.DataArray(darr_scaled,\n",
    "                         dims=ds20.dims,\n",
    "                         coords={'time': ds20.time,\n",
    "                                 'band': ds20.band,\n",
    "                                 'y': new_y,\n",
    "                                 'x': new_x},\n",
    "                         attrs=ds20.attrs)\n",
    "    \n",
    "    ds20u.attrs['resolution'] = new_res\n",
    "    \n",
    "    return ds20u\n",
    "\n",
    "\n",
    "def load_sentinel2_tile(tile,\n",
    "                        start_date,\n",
    "                        end_date,\n",
    "                        max_cloud_cover=90):\n",
    "    \n",
    "    catalog = pystac_client.Client.open(\n",
    "        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "        modifier=planetary_computer.sign_inplace,\n",
    "    )\n",
    "\n",
    "    time_range = f\"{start_date}/{end_date}\"\n",
    "\n",
    "    query_params = {\"eo:cloud_cover\": {\"lt\": max_cloud_cover},\n",
    "                    \"s2:mgrs_tile\": {\"eq\": tile}}\n",
    "\n",
    "    search = catalog.search(collections=[\"sentinel-2-l2a\"],\n",
    "                            datetime=time_range,\n",
    "                            query=query_params)\n",
    "    items = search.get_all_items()\n",
    "\n",
    "    bands = ['B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08',\n",
    "             'B09', 'B11', 'B12', 'B8A', 'SCL']\n",
    "\n",
    "    assets_10m = ['B02', 'B03', 'B04', 'B08']\n",
    "    assets_20m = ['B05', 'B06', 'B07', 'B8A', 'B09',\n",
    "                  'B11', 'B12']\n",
    "    assets_60m = ['B01', 'B09']\n",
    "    scl = 'SCL'\n",
    "\n",
    "    ds = {}\n",
    "    assets = {10: assets_10m,\n",
    "              20: assets_20m,\n",
    "              60: assets_60m,\n",
    "             'scl': [scl]}\n",
    "\n",
    "    chunksize = {10: 1024,\n",
    "                 20: 512,\n",
    "                 60: 512,\n",
    "                 'scl': 512}\n",
    "\n",
    "    dtype = {10: np.uint16,\n",
    "             20: np.uint16,\n",
    "             60: np.uint16,\n",
    "             'scl': np.uint8}\n",
    "\n",
    "    keep_vars = ['time', 'band', 'y', 'x', 'id', 's2:processing_baseline']\n",
    "    for res in assets.keys():\n",
    "        ds[res] = stackstac.stack(items,\n",
    "                                  assets=assets[res],\n",
    "                                  chunksize=chunksize[res],\n",
    "                                  xy_coords='center',\n",
    "                                  rescale=False,\n",
    "                                  dtype=dtype[res],\n",
    "                                  fill_value=0)\n",
    "        del ds[res].attrs['spec']\n",
    "        ds_vars = list(ds[res].coords.keys())\n",
    "        drop_vars = [v for v in ds_vars if v not in keep_vars]\n",
    "        ds[res] = ds[res].drop_vars(drop_vars)\n",
    "        ds[res] = force_unique_time(ds[res])\n",
    "        \n",
    "        # coerce dtypes\n",
    "        for v in ['id', 'band', 's2:processing_baseline']:\n",
    "            ds[res][v] = ds[res][v].astype(str)\n",
    "            \n",
    "    return ds[10], ds[20], ds[60], ds['scl']\n",
    "\n",
    "\n",
    "def get_clusters(dsm, bands=None, n_clusters=20, coarsen=4):\n",
    "    \n",
    "    if bands is None:\n",
    "        bands = ['B02', 'B03', 'B04', 'B08', 'B11', 'B12']\n",
    "\n",
    "    dsk = dsm.sel(band=bands).coarsen(y=coarsen,\n",
    "                                      x=coarsen).mean().persist()\n",
    "\n",
    "    dsk = dsk.chunk((-1, -1, -1, -1))\n",
    "\n",
    "    data = dsk.data.compute()\n",
    "    data = data / 10000\n",
    "    shape = data.shape\n",
    "    data = np.transpose(data, [0, 2, 3, 1]).reshape((-1, shape[1]))\n",
    "\n",
    "    scaler = StandardScaler().fit(data[data[:, 0] > 0, :])\n",
    "    data = scaler.transform(data)\n",
    "\n",
    "    data.shape\n",
    "\n",
    "    n_clusters = 20\n",
    "\n",
    "    k = KMeans(n_clusters).fit(data).labels_\n",
    "    \n",
    "    kim = k.reshape((shape[0], shape[2], shape[3])).astype(np.uint8)\n",
    "    kim = np.expand_dims(kim, 1)\n",
    "\n",
    "    kimd = xr.DataArray(kim, \n",
    "                        dims=('time', 'band', 'y', 'x'),\n",
    "                        coords={'time': dsk.time,\n",
    "                                'band': ['K'],\n",
    "                                'y': dsk.y,\n",
    "                                'x': dsk.x},\n",
    "                        name='kclusters').chunk((-1, -1, -1, -1))\n",
    "    \n",
    "    kimd20 = kimd.satio.rescale_ts(scale=coarsen, order=0)\n",
    "    kimd20 = kimd20.persist()\n",
    "    \n",
    "    return kimd20\n",
    "\n",
    "\n",
    "def get_dark_bright_map(kclusters_block, scl_block):\n",
    "    \n",
    "    n_clusters = kclusters_block.max().values\n",
    "    \n",
    "    k = kclusters_block.isel(band=0).data.compute()\n",
    "    s = scl_block.isel(band=0).data.compute()\n",
    "\n",
    "    s2 = np.ones(s.shape)\n",
    "\n",
    "    dark = 0, 2, 3\n",
    "    bright = 8, 9, 10, 11\n",
    "\n",
    "    s2[np.isin(s, dark)] = 0\n",
    "    s2[np.isin(s, bright)] = 2\n",
    "    s2 = s2.astype(np.uint8)\n",
    "\n",
    "    c = []\n",
    "    for kv in range(n_clusters):\n",
    "        sk = s2[k == kv]\n",
    "        v_sk, n_sk = np.unique(sk, return_counts=True)\n",
    "        c.append(dict(zip(v_sk, n_sk)))\n",
    "\n",
    "    df = pd.DataFrame(c)\n",
    "    df = df.fillna(0)\n",
    "    dfp = df.divide(df.sum(axis=1), axis=0)\n",
    "\n",
    "    dfp.iloc[:, 1] -= 0.2\n",
    "\n",
    "    dark_bright_mapping = dfp.idxmax(axis=1).to_dict()\n",
    "    kdb = np.zeros(s2.shape, dtype=np.uint8)\n",
    "    for i, v in dark_bright_mapping.items():\n",
    "        kdb[k == i] = v\n",
    "\n",
    "    kdb = np.expand_dims(kdb, 1)\n",
    "    kdb = xr.DataArray(kdb, \n",
    "                       dims=('time', 'band', 'y', 'x'),\n",
    "                       coords={'time': scl_block.time,\n",
    "                               'band': ['K2'],\n",
    "                               'y': scl_block.y,\n",
    "                               'x': scl_block.x},\n",
    "                       name='darkbright').chunk((-1, -1, -1, -1))\n",
    "    \n",
    "    return kdb\n",
    "\n",
    "\n",
    "from scipy.ndimage import uniform_filter, gaussian_filter\n",
    "from skimage.filters import sobel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# CDI is not working well with cirrus clouds. Perhaps the cirrus band could help but is\n",
    "# not available, so the CID algorithm can't be fully used. Artefacts are also present.\n",
    "\n",
    "def focal_variance(img, window_size=(1, 7, 7)):\n",
    "    \"\"\"\n",
    "    Calculate the focal variance of the given 2-d image, over a moving window of\n",
    "    size winSize pixels.\n",
    "    \"\"\"\n",
    "    img32 = img.astype(np.float32)\n",
    "    focal_mean = uniform_filter(img32, size=window_size)\n",
    "    mean_sq = uniform_filter(img32 ** 2, size=window_size)\n",
    "    # focal_mean = gaussian_filter(img32, sigma=sigma)\n",
    "    # mean_sq = gaussian_filter(img32 ** 2, sigma=sigma)\n",
    "    variance = mean_sq - focal_mean ** 2\n",
    "    return variance\n",
    "\n",
    "\n",
    "\n",
    "def calc_cdi(darr, window_size):\n",
    "    \"\"\"\n",
    "    Calculate the Cloud Displacement Index, as per Frantz et al (2018).\n",
    "    \"\"\"\n",
    "    # Equations 5 & 6\n",
    "    b07 = np.squeeze(darr.sel(band='B07').data).compute()\n",
    "    b08 = np.squeeze(darr.sel(band='B08').data).compute()\n",
    "    b8a = np.squeeze(darr.sel(band='B8A').data).compute()\n",
    "    \n",
    "    r8 = b08 / b8a\n",
    "    r7 = b07 / b8a\n",
    "    \n",
    "    r8[~np.isfinite(r8)] = 0\n",
    "    r7[~np.isfinite(r7)] = 0\n",
    "    \n",
    "    # Equation 7\n",
    "    v8 = focal_variance(r8, window_size)\n",
    "    v7 = focal_variance(r7, window_size)\n",
    "  \n",
    "    # Mask out where we would divide by zero\n",
    "    cdi = np.zeros(v7.shape, dtype=np.float32)\n",
    "    valid = ((v7 + v8) != 0)\n",
    "    cdi[valid] = (v7[valid] - v8[valid]) / (v7[valid] + v8[valid])\n",
    "\n",
    "    return (r8, r7, v8, v7, cdi)\n",
    "\n",
    "\n",
    "def filter_cdi(bright_mask, cdi):\n",
    "    \n",
    "    selection = bright_mask & (cdi < -0.5)\n",
    "    \n",
    "    # erode selection with 1 px\n",
    "    selection = scipy.ndimage.binary_erosion(selection)\n",
    "    \n",
    "    # region grow within (cdi < -0.25)\n",
    "    rg_mask = bright_mask & (cdi < -0.25)\n",
    "    selection = scipy.ndimage.binary_dilation(selection,\n",
    "                                              mask=rg_mask,\n",
    "                                              iterations=0)\n",
    "    bright_mask[~selection] = False\n",
    "    \n",
    "    return bright_mask\n",
    "\n",
    "\n",
    "def ram_usage():\n",
    "    import psutil\n",
    "    # Getting % usage of virtual_memory ( 3rd field)\n",
    "    print('RAM memory % used:', psutil.virtual_memory()[2])\n",
    "    # Getting usage of virtual_memory in GB ( 4th field)\n",
    "    print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "    \n",
    "\n",
    "# worldcover settings\n",
    "settings = {\n",
    "    \n",
    "    \"l2a\": {\n",
    "        \"bands\": [\"B02\", \"B03\", \"B04\", \"B05\",\n",
    "                  \"B06\", \"B07\", \"B08\", \"B11\", \"B12\"],\n",
    "        \"rsis\": [\"ndvi\", \"nbr\", \"nbr2\", \"evi\",\n",
    "                 \"ndmi\", \"ndwi\", \"ndgi\"],\n",
    "        \"composite\": {\"freq\": 10, \"window\": 20},\n",
    "        \"mask\": {\"erode_r\": 3,\n",
    "                 \"dilate_r\": 13,\n",
    "                 \"mask_values\": [1, 3, 8, 9, 10, 11],\n",
    "                 \"max_invalid_ratio\": 1}},\n",
    "    \n",
    "    \"gamma0\": {\n",
    "        \"bands\": [\"VV\", \"VH\"],\n",
    "        \"rsis\": [\"vh_vv\"],\n",
    "        \"composite\": {\"freq\": 10, \"window\": 20}}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# with open('../../sastoken') as f:\n",
    "#     sas_token = f.read()\n",
    "    \n",
    "# container_client = azure.storage.blob.ContainerClient(\n",
    "#     \"https://dza2.blob.core.windows.net\",\n",
    "#     container_name=\"feats\",\n",
    "#     credential=sas_token,\n",
    "# ) \n",
    "\n",
    "# \"\"\"Algo:\n",
    "# - compute dark and bright features (use blue band for bright and swir for dark)\n",
    "# - join dark and bright pixels with SCL dark and bright + cirrus clouds\n",
    "# - use MNDWI to remove dark water\n",
    "# - use CDI to remove bright buildings\n",
    "# - do erosion - dilation to remove single pixels\n",
    "# - remove outliers based on frequency of dark and bright\n",
    "#     - set threshold on image quality and probability that pixel is dark\n",
    "#       e.g. if image is cloudy, discard pixel, if image is less than x cloud\n",
    "#       and frequency of pixel dark/bright is above thresh vs block mean\n",
    "#       then keep pixel (remove from mask)\n",
    "\n",
    "# - buffer mask\n",
    "\n",
    "\n",
    "# Then we can run the block median value and keep it as ref. for each band?\n",
    "# plus we start rolling and we want to select:\n",
    "#     1. best acquisition closer to window center\n",
    "#         - acquisition score based on block invalid pixels\n",
    "#         - if there is no clear image (cloud less than th=80)\n",
    "#           then check which pixel/acquisition is closest to the median value for the\n",
    "#           target bands, this gives the selected acquisition.\n",
    "#         - store distance from median and cloud % of the acquisition as\n",
    "#           quality metric\n",
    "#     2. when using short windows, we can keep more good obs without cloud cover\n",
    "#        but we will have more noise from artefacts and cloud residuals\n",
    "#        After the timeseries is built, we can filter out outliers based on some\n",
    "#        thresholds of the quality.\n",
    "#        Once these are thresholded out along the time dimension,\n",
    "#        we can run the interpolation to fill in missing values\n",
    "       \n",
    "    \n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db90fcb-25ba-458d-a371-8f171accc543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import ipywidgets as ipw\n",
    "import hvplot.xarray # noqa\n",
    "import hvplot.pandas # noqa\n",
    "import panel as pn\n",
    "import pandas as pd\n",
    "import panel.widgets as pnw\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "@xr.register_dataarray_accessor(\"satio\")\n",
    "class ESAWorldCoverTimeSeries:\n",
    "    def __init__(self, xarray_obj):\n",
    "        self._obj = xarray_obj\n",
    "        self._obj.attrs['bounds'] = self.bounds\n",
    "        # run check that we have a timeseries\n",
    "        # assert xarray_obj.dims == ('time', 'band', 'y', 'x')\n",
    "        \n",
    "    def rescale_ts(self,\n",
    "                   scale=2,\n",
    "                   order=1,\n",
    "                   preserve_range=True,\n",
    "                   nodata_val=0):\n",
    "        return rescale_ts(self._obj,\n",
    "                          scale=scale,\n",
    "                          order=order,\n",
    "                          preserve_range=preserve_range,\n",
    "                          nodata_val=nodata_val)\n",
    "    \n",
    "    def composite(self,\n",
    "                  freq=7,\n",
    "                  window=None,\n",
    "                  start=None,\n",
    "                  end=None,\n",
    "                  use_all_obs=False):\n",
    "        return calculate_moving_composite(self._obj,\n",
    "                                          freq,\n",
    "                                          window,\n",
    "                                          start,\n",
    "                                          end,\n",
    "                                          use_all_obs)\n",
    "    \n",
    "    @property\n",
    "    def bounds(self):\n",
    "        \n",
    "        darr = self._obj\n",
    "        \n",
    "        res = darr.x[1] - darr.x[0]\n",
    "        hres = res / 2\n",
    "        \n",
    "        xmin = (darr.x[0] - hres).values.tolist()\n",
    "        xmax = (darr.x[-1] + hres).values.tolist()\n",
    "        \n",
    "        ymin = (darr.y[-1] - hres).values.tolist()\n",
    "        ymax = (darr.y[0] + hres).values.tolist()\n",
    "        \n",
    "        return xmin, ymin, xmax, ymax\n",
    "    \n",
    "    def harmonize(self):\n",
    "        return harmonize(self._obj)\n",
    "    \n",
    "    def mask(self, mask):\n",
    "        return mask_clouds(self._obj, mask)\n",
    "     \n",
    "    def cache(self, tempdir=None, chunks=None):\n",
    "        tmpfile = tempfile.NamedTemporaryFile(suffix='.nc',\n",
    "                                              prefix='satio-',\n",
    "                                              dir=tempdir) \n",
    "        \n",
    "        chunks = self._obj.chunks if chunks is None else chunks\n",
    "\n",
    "        self._obj.to_netcdf(tmpfile.name)\n",
    "        darr = xr.open_dataarray(tmpfile.name).chunk(chunks)\n",
    "\n",
    "        atexit.register(tmpfile.close)\n",
    "        return darr\n",
    "    \n",
    "    def rgb(self, bands=None, vmin=0, vmax=1000):\n",
    "        bands = ['B04', 'B03', 'B02'] if bands is None else bands\n",
    "\n",
    "        im = self._obj.sel(band=bands).clip(vmin, vmax) / (vmax - vmin)\n",
    "        \n",
    "        return im.interactive.sel(time=pnw.DiscreteSlider).hvplot.rgb(\n",
    "                x='x', y='y', bands='band', data_aspect=1,\n",
    "                flip_yaxis=False, xaxis=False, yaxis=None)\n",
    "    \n",
    "    def plot(self, band=None, vmin=None, vmax=None, colormap='plasma'):\n",
    "        im = self._obj\n",
    "        band = im.band[0] if band is None else band\n",
    "        im = im.sel(band=band)\n",
    "        return im.interactive.sel(time=pnw.DiscreteSlider).plot(vmin=vmin,\n",
    "                                                                vmax=vmax,\n",
    "                                                                colormap=colormap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595eeb0c-655c-47c0-8fcd-68d92d35a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get bright and dark masks based on clusters\n",
    "# from skimage.morphology import binary_erosion, binary_dilation, disk\n",
    "\n",
    "# def binary_erosion_ts(ts, r, band=0):\n",
    "#     d = disk(r)\n",
    "#     for t in range(ts.shape[0]):\n",
    "#         ts[t, band, ...] = binary_erosion(ts[t, band, ...], d)\n",
    "#     return ts\n",
    "\n",
    "# def binary_dilation_ts(ts, r, band=0):\n",
    "#     d = disk(r)\n",
    "#     for t in range(ts.shape[0]):\n",
    "#         ts[t, band, ...] = binary_dilation(ts[t, band, ...], d)\n",
    "#     return ts\n",
    "\n",
    "\n",
    "# n_clusters = 20\n",
    "\n",
    "# # bright map\n",
    "# kim = get_clusters(dsm20, bands=['B02', 'B03'],\n",
    "#                    n_clusters=n_clusters)\n",
    "# bright = get_dark_bright_map(kim, scl_block)\n",
    "# bright = bright == 2\n",
    "\n",
    "# n_clusters = 20\n",
    "\n",
    "# # bright map\n",
    "# kim = get_clusters(dsm20, bands=['B03', 'B08', 'B12'], n_clusters=n_clusters)\n",
    "# dark = get_dark_bright_map(kim, scl_block)\n",
    "# dark = dark == 0\n",
    "\n",
    "# bright['band'] = ['bright']\n",
    "# dark['band'] = ['dark']\n",
    "\n",
    "# dark_vals = 2, 3\n",
    "# bright_vals = 8, 9, 10, 11\n",
    "\n",
    "# scl_nodata = (scl_block.data == 0).compute()\n",
    "# scl_bright = np.isin(scl_block.data, bright_vals).compute()\n",
    "# scl_dark = np.isin(scl_block.data, dark_vals).compute()\n",
    "\n",
    "# merged_dark = dark | scl_dark\n",
    "# merged_bright = bright | scl_bright\n",
    "\n",
    "# merged_dark = binary_erosion_ts(merged_dark, 2)\n",
    "# merged_dark = binary_dilation_ts(merged_dark, 5)\n",
    "\n",
    "# merged_bright = binary_erosion_ts(merged_bright, 1)\n",
    "# merged_bright = binary_dilation_ts(merged_bright, 10)\n",
    "\n",
    "# merged_mask = merged_dark.satio.rescale_ts(scale=2, order=0).data | merged_bright.satio.rescale_ts(scale=2, order=0).data\n",
    "\n",
    "# mask = np.broadcast_to(merged_mask.compute(), im2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dfd28d-f732-4b09-9f4a-8c2bf7fd2244",
   "metadata": {},
   "outputs": [],
   "source": [
    "ram_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a56aa73-20ff-43c2-b34a-64bb88b2725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from satio_pc.grid import get_blocks_gdf\n",
    "\n",
    "year = 2022\n",
    "tile_id = '31UFS'\n",
    "\n",
    "start_date = f'{year}-01-01'\n",
    "end_date = f'{year}-05-01'\n",
    "max_cloud_cover = 90\n",
    "\n",
    "blocks_gdf = get_blocks_gdf([tile_id])\n",
    "block = blocks_gdf.iloc[0]\n",
    "block\n",
    "\n",
    "ds10, ds20, ds60, scl = load_sentinel2_tile(tile_id,\n",
    "                                        start_date,\n",
    "                                        end_date,\n",
    "                                        max_cloud_cover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d42140-654b-4e16-a6bc-2a696032786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cfb2dc-1827-4c72-8b13-3bb4c46e6669",
   "metadata": {},
   "outputs": [],
   "source": [
    "scl_settings = settings['l2a']['mask']\n",
    "\n",
    "keys = \"erode_r, dilate_r, max_invalid_ratio, mask_values\".split(', ')\n",
    "erode_r, dilate_r, max_invalid_ratio, mask_values = [scl_settings[k]\n",
    "                                                     for k in keys]\n",
    "\n",
    "scl_post = scl_to_mask(scl,\n",
    "                       mask_values=mask_values,\n",
    "                       erode_r=erode_r,\n",
    "                       dilate_r=dilate_r,\n",
    "                       max_invalid_ratio=max_invalid_ratio)\n",
    "scl20 = scl_post.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4843948-e4b7-47f0-9668-6cf3761fa2c2",
   "metadata": {},
   "source": [
    "### Block processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d67851-7808-428c-81ef-f3aca98cbf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds10_block = ds10.rio.clip_box(*block.bounds).satio.harmonize()\n",
    "ds20_block = ds20.rio.clip_box(*block.bounds).satio.harmonize()\n",
    "scl20_block = scl20.rio.clip_box(*block.bounds)\n",
    "\n",
    "# ds60_block = ds60.rio.clip_box(*block.bounds).satio.harmonize()\n",
    "# ds60_block = ds60_block.satio.cache('.', (-1, -1, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2e9e0e-3eb0-4385-945d-c9969cd7ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds10_block = ds10_block.satio.cache('.', (-1, -1, 256, 256))\n",
    "ds20_block = ds20_block.satio.cache('.', (-1, -1, 256, 256))\n",
    "scl20_block = scl20_block.satio.cache('.', (-1, -1, 256, 256))\n",
    "\n",
    "scl_block = scl.rio.clip_box(*block.bounds).satio.cache('.', (-1, -1, 256, 256))\n",
    "scl10_block = scl20_block.satio.rescale_ts(scale=2, order=0)\n",
    "scl10_block = scl10_block.satio.cache('.', (-1, -1, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa9298-0ebb-4921-94da-804f4488fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds20_block_10m = ds20_block.satio.rescale_ts(scale=2,\n",
    "                                             order=1)\n",
    "dsm10 = xr.concat([ds10_block, ds20_block_10m], dim='band').satio.cache('.', (-1, -1, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe3dcb1-340e-40d1-b51f-1edd569e451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds10_block_20m = ds10_block.satio.rescale_ts(scale=0.5, order=1)\n",
    "\n",
    "dsm20 = xr.concat([ds10_block_20m, ds20_block], dim='band').satio.cache('.', (-1, -1, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f211e16e-65b2-4ba2-900e-5e3e6b52b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "ram_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91fd8c2-2d0a-4dac-8215-219b76f8c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm10.satio.rgb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177af86c-fed2-4094-aaad-6f3dd7a53838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# im3 = im2.where(np.broadcast_to(kdb10, im2.shape) == 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343fce7f-e705-4dcf-8124-b560ac1b07d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MNDWI\n",
    "\n",
    "# bands = ['B04', 'B03', 'B02']\n",
    "\n",
    "# vmin = 0\n",
    "# vmax = 1500\n",
    "\n",
    "# b03 = dsm.sel(band=['B02']).data / 10000\n",
    "# b12 = dsm.sel(band=['B12']).data / 10000\n",
    "\n",
    "# mndwi = (b03 - b12) / (b03 + b12)\n",
    "\n",
    "# mndwi = dsm.sel(band=['B03']).copy(data=mndwi).persist()\n",
    "\n",
    "# mndwi.isel(band=0).interactive.sel(time=pnw.DiscreteSlider).hvplot(aspect=1,\n",
    "#                                                                    colormap='PiYG',\n",
    "#                                                                    clim=(-1, 1))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10801cd-6274-4518-bd52-798c0b892739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79eaaae-701b-41ff-94b8-d6f7d5cd503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm10 = dsm10.satio.mask(scl10_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06dc08a-4b19-4577-9527-6ae4e4649f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea38052-91a8-43bc-9af3-e04a8f160c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm10.satio.rgb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17104954-fb1c-4c9d-a695-847cabc7ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds10_block = ds10_block.satio.mask(scl10_block.isel(band=0).data)\n",
    "ds20_block = ds20_block.satio.mask(scl20_block.isel(band=0).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec7da9-4737-4de9-bdc2-3fd4b05ea48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds10_block = ds10_block.satio.composite(\n",
    "    freq=10,\n",
    "    window=20,\n",
    "    start='2022-01-01',\n",
    "    end='2022-04-01').satio.cache('.', (-1, -1, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cac846-9bec-4555-aba9-c5761a548e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds20_block = ds20_block.satio.composite(\n",
    "    freq=10,\n",
    "    window=20,\n",
    "    start='2022-01-01',\n",
    "    end='2022-04-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e967fd-85db-48aa-80ad-158a4da247cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds20_block = ds20_block.satio.rescale_ts(scale=2, order=1).satio.cache('.', (-1, -1, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feef375f-67c8-44b8-be93-6dc1942a5887",
   "metadata": {},
   "outputs": [],
   "source": [
    "ram_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d677c7-5651-441f-9884-e0a6dabd3c9b",
   "metadata": {},
   "source": [
    "### Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dda22fc-3e53-427e-a347-3b23776664ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import guvectorize\n",
    "\n",
    "\n",
    "def _interp1d(xnew, xvals, yvals, ynew):\n",
    "    i = 0\n",
    "    N = len(xvals)\n",
    "    if xnew[0] < xvals[0]:\n",
    "        # x_a = 0.0\n",
    "        # y_a = 0.0\n",
    "        # x_b = xvals[0]\n",
    "        # y_b = yvals[0]\n",
    "        ynew[0] = yvals[0]\n",
    "        return\n",
    "    elif xnew[-1] > xvals[-1]:\n",
    "        ynew[-1] = yvals[-1]\n",
    "        return\n",
    "    else:\n",
    "        while xnew[0] >= xvals[i] and i < N:\n",
    "            i += 1\n",
    "        if xnew[0] == xvals[i]:\n",
    "            ynew[0] = yvals[i]\n",
    "            return\n",
    "        if i == N:\n",
    "            i = N-1\n",
    "        x_a = xvals[i-1]\n",
    "        y_a = yvals[i-1]\n",
    "        x_b = xvals[i]\n",
    "        y_b = yvals[i]\n",
    "    slope = (xnew[0] - x_a)/(x_b - x_a)\n",
    "    ynew[0] = slope * (y_b-y_a) + y_a\n",
    "    return\n",
    "\n",
    "\n",
    "interp1d = guvectorize(\n",
    "    ['int64[:], int64[:], float32[:], float32[:]'],\n",
    "    \"(),(n),(n) -> ()\", nopython=True)(_interp1d)\n",
    "\n",
    "interp1d_uint16 = guvectorize(\n",
    "    ['int64[:], int64[:], uint16[:], uint16[:]'],\n",
    "    \"(),(n),(n) -> ()\", nopython=True)(_interp1d)\n",
    "\n",
    "\n",
    "def interpolate_fast(arrs):\n",
    "\n",
    "    for band in range(arrs.shape[0]):\n",
    "        for px in range(arrs.shape[2]):\n",
    "            y = arrs[band, :, px]\n",
    "\n",
    "            nans_ids = np.isnan(y)\n",
    "            xvals = np.where(~nans_ids)[0]\n",
    "            yvals = y[xvals]\n",
    "            xnew = np.where(nans_ids)[0]\n",
    "            y[xnew] = interp1d(xnew, xvals, yvals)\n",
    "\n",
    "    return arrs\n",
    "\n",
    "\n",
    "def _interpolate_4d_float32(arrs):\n",
    "\n",
    "    for band in range(arrs.shape[0]):\n",
    "        for px in range(arrs.shape[2]):\n",
    "            for py in range(arrs.shape[3]):\n",
    "                y = arrs[band, :, px, py]\n",
    "\n",
    "                nans_ids = np.isnan(y)\n",
    "                xvals = np.where(~nans_ids)[0]\n",
    "                yvals = y[xvals]\n",
    "                xnew = np.where(nans_ids)[0]\n",
    "                y[xnew] = interp1d(xnew, xvals, yvals)\n",
    "\n",
    "    return arrs\n",
    "\n",
    "\n",
    "def _interpolate_4d_uint16(darr):\n",
    "\n",
    "    for band in range(darr.shape[1]):\n",
    "        for py in range(darr.shape[2]):\n",
    "            for px in range(darr.shape[3]):\n",
    "                # t = darr.isel(band=band,\n",
    "                #               y=px,\n",
    "                #               x=py).data\n",
    "                t = darr[:, band, py, px]\n",
    "\n",
    "                nans_ids = (t == 0)\n",
    "                xvals = np.where(~nans_ids)[0]\n",
    "                yvals = t[xvals]\n",
    "                xnew = np.where(nans_ids)[0]\n",
    "                t[xnew] = interp1d(xnew, xvals, yvals)\n",
    "\n",
    "    return darr\n",
    "\n",
    "\n",
    "def interpolate_4d(arrs):\n",
    "\n",
    "    if arrs.dtype == np.float32:\n",
    "        return _interpolate_4d_float32(arrs)\n",
    "    elif arrs.dtype == np.uint16:\n",
    "        return _interpolate_4d_uint16(arrs)\n",
    "    else:\n",
    "        raise ValueError(\"Interpolate function is only available for \"\n",
    "                         \"arrays of type float32 or uint16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b21c4d-e0ab-4ca3-9317-c27536b6159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds10_block.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45123855-0513-428e-91b7-851ddfd71c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "i = interpolate_4d(ds10_block.compute().data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c8df6d-1456-4f19-b541-3a788f6cf430",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds20_block_i = ds20_block.copy(data=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38df387-e21b-4033-bb09-cdd8a06cb9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e05c4b-d995-47ab-96c3-a284807b7104",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds20_block_10 = ds20_block.satio.to_10m(bounds=block.bounds,\n",
    "                                        order=3).persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a85588d-571a-4548-979f-6efda6271a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2484292-73c2-4355-ad5a-b2b4dbd4a9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds20_block_masked_comp = ds20_block_masked_comp.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454736b1-efb7-4a46-8e50-9f08b59d0f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as ipw\n",
    "import hvplot.xarray # noqa\n",
    "import hvplot.pandas # noqa\n",
    "import panel as pn\n",
    "import pandas as pd\n",
    "import panel.widgets as pnw\n",
    "import xarray as xr\n",
    "\n",
    "rgb_bands = ['B04', 'B03', 'B02']\n",
    "\n",
    "im = ds10_block.sel(band=rgb_bands).clip(0, lim) / lim\n",
    "\n",
    "im.interactive.sel(time=pnw.DiscreteSlider).hvplot.rgb(\n",
    "    x='x', y='y', bands='band', data_aspect=1,\n",
    "    flip_yaxis=True, xaxis=False, yaxis=None)\n",
    "# ds10_block.sel(band=rgb_bands).interactive.sel(time=pnw.DiscreteSlider).plot(vmin=0, vmax=2500)\n",
    "\n",
    "# ds10_block.sel(band=rgb_bands).isel(time=0).hvplot.rgb(x='x', y='y', bands='band')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec389e-0caa-40c0-a0c2-90988ef9347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lim = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c78d7-5c24-49d4-a76c-ebfc6e316daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = ds10_block.sel(band=rgb_bands).clip(0, lim) / lim\n",
    "\n",
    "im.interactive.sel(time=pnw.DiscreteSlider).hvplot.rgb(\n",
    "    x='x', y='y', bands='band', data_aspect=1,\n",
    "    flip_yaxis=True, xaxis=False, yaxis=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61712d4c-0cf8-48ab-92a7-423d788c7e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
